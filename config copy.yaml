default_model: phi-2
lightweight_model: technique-detector
log_dir: ./logs
model_registry:
  clip-vit-base: openai/clip-vit-base-patch32
  distilbert-base: distilbert-base-uncased
  falcon-7b: ./models/falcon-7b
  gemma-7b: ./models/gemma-7b
  hf-mixtral-7b: mistralai/Mistral-7B-Instruct-v0.1
  llama-3-8b: meta-llama/Meta-Llama-3-8B
  mistral-7b: ./models/mistral-7b
  mixtral-8x7b: mistralai/Mixtral-8x7B-v0.1
  mpt-7b: ./models/mpt-7b
  phi-2: ./models/phi-2
  vit-base-classification: google/vit-base-patch16-224
  vit-gpt2-captioning: nlpconnect/vit-gpt2-image-captioning
models:
- cost_per_token: 0.0
  name: mistral-7b
  parameters:
    context_length: 4096
    model_id: mistral-7b
    model_registry: ${model_registry}
    temperature: 0.7
  provider: local
  strengths:
  - general_knowledge
  - coding
  - reasoning
- cost_per_token: 0.0
  name: llama-3-8b
  parameters:
    context_length: 8192
    model_id: llama-3-8b
    model_registry: ${model_registry}
    temperature: 0.7
  provider: local
  strengths:
  - general_knowledge
  - creative
  - long_context
  - reasoning
- cost_per_token: 0.0
  name: phi-2
  parameters:
    context_length: 2048
    model_id: phi-2
    model_registry: ${model_registry}
    temperature: 0.7
  provider: local
  strengths:
  - reasoning
  - math
  - coding
- cost_per_token: 0.0
  name: gemma-7b
  parameters:
    context_length: 8192
    model_id: gemma-7b
    model_registry: ${model_registry}
    temperature: 0.7
  provider: local
  strengths:
  - general_knowledge
  - math
  - reasoning
- cost_per_token: 0.0
  name: mpt-7b
  parameters:
    context_length: 2048
    model_id: mpt-7b
    model_registry: ${model_registry}
    temperature: 0.7
  provider: local
  strengths:
  - general_knowledge
  - creative
- cost_per_token: 0.0
  name: falcon-7b
  parameters:
    context_length: 2048
    model_id: falcon-7b
    model_registry: ${model_registry}
    temperature: 0.7
  provider: local
  strengths:
  - general_knowledge
  - summarization
- api_key_env: HUGGINGFACE_API_KEY
  cost_per_token: 0.0
  name: mixtral-8x7b
  parameters:
    max_tokens: 4096
    model_id: mixtral-8x7b
    model_registry: ${model_registry}
    temperature: 0.7
  provider: huggingface
  strengths:
  - general_knowledge
  - coding
  - reasoning
  - creative
- api_key_env: HUGGINGFACE_API_KEY
  cost_per_token: 0.0
  name: hf-mixtral-7b
  parameters:
    max_tokens: 4096
    model_id: mistralai/Mistral-7B-Instruct-v0.1
    model_registry: ${model_registry}
    temperature: 0.7
  provider: huggingface
  strengths:
  - general_knowledge
  - coding
  - reasoning
  - creative
- cost_per_token: 0.0
  name: technique-detector
  parameters:
    max_tokens: 50
    model_id: phi-2
    model_registry: ${model_registry}
    temperature: 0.3
  provider: local
  strengths:
  - classification
  - short_context
parameters:
  cache_size: 100
  classification_model: technique-detector
  lightweight_model: technique-detector
  open_source_only: true
routing_strategy: rule_based
